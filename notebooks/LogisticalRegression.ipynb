{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Logistička regresija"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from testbench import Testbench\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Testbench.author('Petar Petrović')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Učitavanje podataka\n",
    "\n",
    "Dataset sadrži dve komponente koje nam daje PCA (nazvane su PC1 i PC2) i kategoriju težine (nazvana je NObeyesdad, preimenovaćemo je u Obese) koju koristimo kao labelu. Težina ima više vrednosti koje su pretvorene u numeričke (0-3) i predstavljaju redom neuhranjenost, normalnu težinu, prekomernu težinu i gojaznost. Pošto je logistička regresija binarni klasifikator, radićemo predikciju samo da li je osoba gojazna ili ne, odnosno prve 3 kategorije ćemo spojiti u jednu (nije gojazna). "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def load_data(name: str):\n",
    "    df = pd.read_csv(name)\n",
    "    clean_df = pd.DataFrame()\n",
    "    \n",
    "    # PC1 i PC2 kolone samo kopiramo\n",
    "    for col in [\"PC1\", \"PC2\"]:\n",
    "        clean_df[col] = df[col]\n",
    "        \n",
    "    # sa proverom jednakosti dobijamo True/False, pretvaranjem u int dobijamo 1/0 sto nam treba\n",
    "    targets = (df[\"NObeyesdad\"] == 3).astype(int).rename(\"Obese\")\n",
    "\n",
    "    return clean_df, targets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "features, labels = load_data(\"../datasets/obesity_pca.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vizualizacija podataka\n",
    "\n",
    "Za početak možemo pogledati kako izgledaju naši podaci u 2D prostoru. Na osama su vrednosti izdvojenih komponenti, svaka tačka je jedna osoba iz dataseta a njena boja pokazuje da li je gojazna (narandžasta ako jeste ili plava ako nije)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "data = pd.concat([features, labels], axis=1)\n",
    "sns.scatterplot(data=data, x=\"PC1\", y=\"PC2\", hue=\"Obese\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hipoteza\n",
    "\n",
    "Hipoteza je glavni deo našeg modela, to je funkcija koja na osnovu težina (koje će naš algoritam da \"nauči\") i featura osobe vraća \"mišljenje\" našeg modela o gojaznosti te osobe. Hipoteza vraća realan broj između 0 i 1, a tek nakon treniranja ćemo ga pretvoriti u klasu (0 ako je ispod 0.5, 1 ako je iznad). To nam omogućava da model postepeno uči, jer ne mora da pravi skokove između 0 i 1 već može polako da prelazi.\n",
    "\n",
    "Nakon što se vrednosti featura pomnože sa odgovarajućim težinama i kad se te vrednosti saberu, primenjujemo sigmoidnu funkciju. Njen izlaz je uvek između 0 i 1 (najveće negativne vredosti idu u 0 a pozitivne u 1) što nam i treba."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# X - lista featura osoba\n",
    "# theta - težine\n",
    "# funkcija vraća listu hipoteza, za svaku osobu po jednu predikciju\n",
    "def hypothesis(X, theta):\n",
    "    # mnozenje vrednosti featura sa tezinama i njihovo sabiranje, predstavljeno u obliku matricnog proizvoda\n",
    "    z = np.dot(theta, X.T)\n",
    "    # ne zelimo da dobijemo tacno 0 ili 1 jer ce nam kasnije to praviti problem u logaritmu,\n",
    "    # zato se ogranicimo na opseg od 0.000001 do 0.9999999\n",
    "    return np.clip(1/(1+np.exp(-(z))), 0.000001, 0.9999999)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cost funkcija\n",
    "\n",
    "Cost funkcija (nekad se zove i loss funckija) je funkcija čiji minimum ćemo tražiti, i treba da predstavlja grešku hipoteze tj. razliku predviđenih i stvarnih klasa na celom datasetu. Minimizacijom ove funkcije minimizovaće se i ta razlika tj. greška modela.\n",
    "\n",
    "Najjednostavnije bi bilo uzeti prosečnu apsolutnu vrednost razlike predviđanja i stvarnih labela, međutim postoje i druge funkcije sa kojima se postiže brža konvergencija algoritma (brži dolazak do minimuma). Neke od često korišćenih su mean square error (MSE) - umesto apsolutne vrednosti razlike koristi se njen kvadrat; i cross-entropy, koja u suštini predstavlja prosečan logaritam greške."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# y - stvarne klase osoba\n",
    "# y1 - hipoteze\n",
    "# funkcija vraća prosecan cost na celom datasetu\n",
    "def cost(y, h):\n",
    "    # racunamo cross entropy\n",
    "    return -(1/len(y)) * np.sum(y*np.log(h) + (1-y)*np.log(1-h))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradient descent\n",
    "\n",
    "Glavni deo logističke regresije koji zapravo \"uči\" težine. Radi tako što izračuna gradijent costa po theta i \"pomeri\" theta u pravcu tog gradijenta. Koliko će se pomeriti zavisi od learning rate-a, a koliko puta će ponoviti taj postupak zavisi od zadatog broja iteracija. Veći learning rate ubrzava prilaženje minimuma ali takođe i povećava šansu da će se minimum preskočiti. Veći broj iteracija produžava vreme izvršavanja algoritma ali dozvoljava da se više približimo minimumu (ako algoritam nije iskonvergirao u ranijim iteracijama).\n",
    "\n",
    "### Zadatak\n",
    "\n",
    "Treba popuniti označenu liniju tako da se kompletira gradient descent.\n",
    "\n",
    "**Pomoć:** Parcijalni izvod costa po `theta_i` je `1/m * sum((h[j] - y[j]) * X[j][i])` (`m` je broj redova u `X`; suma je po `j` od `0` do `m`)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# X - lista featura osoba\n",
    "# y - stvarne klase osoba\n",
    "# theta - pocetne tezine\n",
    "# alpha - learning rate koeficijent\n",
    "# epochs - broj iteracija\n",
    "# funkcija vraca vrednosti costa kroz iteracije i naucene theta koeficijente\n",
    "def gradient_descent(X, y, theta, alpha, epochs):\n",
    "    # racunanje pocetnog costa\n",
    "    h = hypothesis(X, theta)\n",
    "    cost_history = [cost(y, h)]\n",
    "    \n",
    "    m = len(X)\n",
    "    for _ in range(0, epochs):\n",
    "        h = hypothesis(X, theta)\n",
    "        for i in range(0, len(theta)):\n",
    "            # updateovanje theta[i]\n",
    "            ##############\n",
    "            # TODO Popuni\n",
    "            ##############\n",
    "            \n",
    "        # cuvanje costa u trenutnoj iteraciji\n",
    "        cost_history.append(cost(y, h))\n",
    "    return cost_history, theta"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testiranje rešenja"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def logistical_regression(X, y, theta, alpha, epochs):\n",
    "    return gradient_descent(X, y, theta, alpha, epochs)\n",
    "Testbench(logistical_regression);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predviđanje\n",
    "\n",
    "Da bismo dobili predviđanje modela, treba samo da izračunamo hipotezu sa istreniranim koeficijentima i da zaokružimo vrednosti na 0 ili 1."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# X - lista featura osoba\n",
    "# theta - težine\n",
    "# funkcija vraća listu predikcija, za svaku osobu po jednu predikciju\n",
    "def predict(X, theta):\n",
    "    h = hypothesis(X, theta)\n",
    "    for i in range(len(h)):\n",
    "        h[i]=1 if h[i]>=0.5 else 0\n",
    "    return h"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pokretanje algoritma\n",
    "\n",
    "Sve što je preostalo je da istreniramo model, dobijemo predikcije i izračunamo tačnost modela. Za početne vrednosti koeficijenata smo potpuno nasumično uzeli 0.5, a za learning rate i broj iteracija su uzete vrednosti koje za ovaj dataset daju prihavtljivu tačnost i vreme izvršavanja. Te parametre možete menjati i videti njihov uticaj, a u sledećem odeljku je i objašnjenje kako možete doći do još boljeg izbora parametara."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# pocetne vrednosti koeficijenata\n",
    "theta = [0.5]*len(features.columns)\n",
    "# ucenje koeficijenata\n",
    "cost_history, theta = gradient_descent(features, labels, theta, 0.0001, 500)\n",
    "# predikcija\n",
    "y_pred = predict(features, theta)\n",
    "\n",
    "# racunanje tacnosti\n",
    "y = list(labels)\n",
    "acc = np.sum([y[i] == y_pred[i] for i in range(len(y))])/len(y)\n",
    "print(\"Tačnost:\", acc * 100, \"%\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vizualizacija greške\n",
    "\n",
    "Jedan način provere da li algoritam dobro radi je da vidimo kako se greška (tj. vrednost cost funkcije) menja tokom iteracija. Greška bi uvek trebalo da opada kroz iteracije ako je gradient descent dobro implementiran.\n",
    "\n",
    "Ovaj grafik nam takođe može pomoći da odredimo vrednosti za learning rate i broj iteracija. Ako greška previše sporo opada, to je znak da se learning rate možda može povećati, a ako brzo opada i konvergira, treba probati da se smanji (što može dovesti do nalaženja boljeg minimuma). Što se tiče broja iteracija, ako vidimo da greška i u poslednjim iteracijama nastavlja da opada onda ima smisla povećati broj iteracija, a ako vrlo brzo prestane da se smanjuje mogao bi se smanjiti broj i time se smanjiti dužina treniranja bez velikog uticaja na grešku.\n",
    "\n",
    "Ovaj \"recept\" je samo okviran, na primer premali learning rate i premali broj iteracija daju sličan grafik (ne znate da li greška sporo opada zato što je learning rate premali, ili zato što je problem \"težak\" i jednostavno je potrebno više iteracija). Jedno rešenje je da se fiksira broj iteracija i prvo odredi optimalan learning rate pa tek onda da se menja broj iteracija."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize = (12, 8))\n",
    "plt.plot(range(len(cost_history)), cost_history)\n",
    "plt.ylabel(\"Greška\")\n",
    "plt.xlabel(\"Iteracija\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}